

・**TimeCMA** TimeCMA: Towards LLM-Empowered Multivariate Time Series Forecasting via Cross-Modality Alignment [[AAAI 2025]](https://arxiv.org/abs/2406.01638) [[Code]](https://github.com/ChenxiLiu-HNU/TimeCMA.git)

・**AutoTimes** AutoTimes: Autoregressive Time Series Forecasters via Large Language Models [[NeurIPS 2024]](https://arxiv.org/abs/2402.02370) [[Code]](https://github.com/thuml/AutoTimes.git)

・**Time-LLM** Time-LLM: Time Series Forecasting by Reprogramming Large Language Models [[ICLR 2024]](https://arxiv.org/abs/2310.01728) [[Code]](https://github.com/KimMeen/Time-LLM.git)

・**OFA** One Fits All:Power General Time Series Analysis by Pretrained LM [[NeurIPS 2023]](https://arxiv.org/abs/2302.11939) [[Code]](https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All.git)

・**LLMTime** Large Language Models Are Zero-Shot Time Series Forecasters [[NeurIPS 2023]](https://arxiv.org/abs/2310.07820) [[Code]](https://github.com/ngruver/llmtime.git)

・**TEMPO** TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting [[ICLR 2024]](https://arxiv.org/abs/2310.04948) [[Code]](https://github.com/DC-research/TEMPO.git)

・**TimesFM** A decoder-only foundation model for time-series forecasting [[ICML 2024]](https://arxiv.org/abs/2310.10688) [[Code]](https://github.com/google-research/timesfm.git)

・**AutoTimes** Autoregressive Time Series Forecasters via Large Language Models [[NeurIPS 2024]](https://arxiv.org/abs/2402.02370) [[Code]](https://github.com/thuml/AutoTimes.git)

・**GPT4MTS** Prompt-based Large Language Model for Multimodal Time-Series Forecasting [[AAAI 2024]](https://ojs.aaai.org/index.php/AAAI/article/view/30383) [[Code]](https://github.com/Flora-jia-jfr/GPT4MTS-Prompt-based-Large-Language-Model-for-Multimodal-Time-series-Forecasting.git)

・**ChatTime** ChatTime: A Unified Multimodal Time Series Foundation Model Bridging Numerical and Textual Data [[AAAI 2025]](https://arxiv.org/abs/2412.11376) [[Code]](https://github.com/ForestsKing/ChatTime.git)

・**S²IP-LLM** S²IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting [[ICML 2024]](https://arxiv.org/abs/2403.05798) [[Code]](https://github.com/panzijie825/S2IP-LLM.git)

・**aLLM4TS** Multi-Patch Prediction: Adapting LLMs for Time Series Representation Learning [[ICML 2024]](https://arxiv.org/abs/2402.04852) [[Code]](https://github.com/yxbian23/aLLM4TS.git)

・**ST-LLM** ST-LLM: Large Language Models Are Effective Temporal Learners [[MDM 2024]](https://arxiv.org/abs/2404.00308) [[Code]](https://github.com/ChenxiLiu-HNU/ST-LLM.git)

・**Moment** MOMENT: A Family of Open Time-series Foundation Model [[ICML 2024]](https://arxiv.org/abs/2402.03885) [[Code]](https://github.com/moment-timeseries-foundation-model/moment.git)

・**Timer** Timer: Generative Pre-trained Transformers Are Large Time Series Models [[ICML 2024]](https://arxiv.org/abs/2402.02368) [[Code]](https://github.com/moment-timeseries-foundation-model/moment.git)

・**Moirai** Unified Training of Universal Time Series Forecasting Transformers [[ICML 2024]](https://arxiv.org/abs/2402.02592) [[Code]](https://github.com/SalesforceAIResearch/uni2ts.git)

・**Timer-XL** Timer-XL: Long-Context Transformers for Unified Time Series Forecasting [[ICLR 2025]](https://arxiv.org/abs/2410.04803) [[Code]](https://github.com/thuml/Timer-XL.git)

・**Time-MoE** Time-MoE: Billion-Scale Time Series Foundation Models With Mixture Of Experts [[ICLR 2025]](https://arxiv.org/abs/2409.16040) [[Code]](https://github.com/Time-MoE/Time-MoE.git)

・**Chronos** Chronos: Learning the Language of Time Series [[TMLR 2024]](https://arxiv.org/abs/2403.07815) [[Code]](https://github.com/amazon-science/chronos-forecasting.git)

・**LMTraj** Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction [[CVPR 2024]](https://arxiv.org/abs/2403.18447) [[Code]](https://github.com/InhwanBae/LMTrajectory.git)

・**ICTSP** In-context Time Series Predictor [[ICLR 2025]](https://arxiv.org/abs/2403.18447) [[Code]](https://github.com/LJC-FVNR/In-context-Time-Series-Predictor.git)










